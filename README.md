# Detecting-LLM-Prompt-Injections-and-Jailbreaks-in-2024

This notebook uses some techniques reported in 2023 on how to avoid jailbreaks and prompt insertions of LLMs.

Malicious attacks can cause the LLM to create different behavious than intended:
- producing hate speech,
- creating misinformation,
- leaking private information, or
- misleading the model into performing other different task than intended.

This notebook looks at two types of methods to perform malicious attacks:
- jailbreaks and
- prompt injections
